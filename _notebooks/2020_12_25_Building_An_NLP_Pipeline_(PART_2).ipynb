{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZ-5UTVSPcCG"
   },
   "source": [
    "# \"Topic 02.2: Building an NLP Pipeline(PART-2)\"\n",
    "\"Introduction to NLP pipeline\"\n",
    "\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [nlp-pipeline]\n",
    "- image: images/logo/logo.png\n",
    "- hide: False\n",
    "- sticky_rank: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXMr9KH1aioY"
   },
   "source": [
    "<img src = \"my_icons/topic_02.a.1.png\">\n",
    "\n",
    "\n",
    "Let's resume our discussion on NLP pipeline. In the previous blog post we completed the first 2 steps of NLP pipeline that is Data aquisition and text cleaning. In this blog post we will cover data pre-processing and feature engineering.\n",
    "\n",
    "# PRE-PROCESSING:  \n",
    "\n",
    "To pre-process your text simply means to bring your text into a form that is predictable  and analysable for your task. A task here is a combination of approach and domain. For example, extracting top keywords with TF-IDF (approach) from Tweets (domain) is an example of a Task. \n",
    "\n",
    " \n",
    "                      Task = approach + domain \n",
    "\n",
    "\n",
    "One task’s ideal pre-processing can become another task’s worst nightmare. So take note: text pre-processing is not directly transferable from task to task. \n",
    "\n",
    "Let’s take a very simple example, let’s say you are trying to discover commonly used words in a news dataset. If your pre-processing step involves removing stopwords  because some other task used it, then you are probably going to miss out on some of the common words as you have ALREADY eliminated it. So really, it’s not a one-size-fits-all approach. \n",
    "\n",
    "Here are some common pre-processing steps used in NLP software: \n",
    "\n",
    "* Preliminaries:  \n",
    "Sentence segmentation and word tokenization. \n",
    "\n",
    " \n",
    "* Frequent steps:  \n",
    "Stop word removal, stemming and lemmatization, removing digits/punctuation, lowercasing, etc. \n",
    "\n",
    " \n",
    "\n",
    "* Advanced processing:  \n",
    "  POS tagging, parsing, coreference resolution, etc. \n",
    "\n",
    " \n",
    "\n",
    "## Preliminaries :  \n",
    "\n",
    "While not all steps will be followed in all the NLP pipelines we encounter, the first two are more or less seen everywhere. Let’s take a look at what each of these steps mean. \n",
    "\n",
    " \n",
    "\n",
    "The NLP can analysis the text by breaking it into sentence(sentence segmentation) and then further into words(words tokenization). \n",
    "\n",
    "### SENTENCE SEGMENTATION : \n",
    "\n",
    "We may easily divide the text into sentence on the basis of the position of the full stop(.). But what happen if we have Dr.Joy or (….) in our text. \n",
    "\n",
    "We have NLP libraries which help to overcome these issue. Like NLTK. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlQrtjqW4qYn",
    "outputId": "61897278-5539-4f38-a00b-eedf7cc5662b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9YPsY1mpbzdI",
    "outputId": "b1a00bbe-f8cd-4c52-d013-bac1554da263"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[\"It's fun to study NLP.\", 'Would recommend to all.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"It's fun to study NLP. Would recommend to all.\"\n",
    "print(sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjRTtqQnchGW"
   },
   "source": [
    "### WORD TOKENIZATION : \n",
    "\n",
    "To tokenize a sentence into words, we can start with a simple rule to split text into words based on the presence of punctuation marks. The NLTK library allows us to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HurCWQeCcrkD",
    "outputId": "14d20a55-a0d4-433c-f4d9-d13a4c73d163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'fun', 'to', 'study', 'NLP', '.', 'Would', 'recommend', 'to', 'all', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"It's fun to study NLP. Would recommend to all.\"\n",
    "print(word_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZgAPJycc510"
   },
   "source": [
    "## Frequent Steps : \n",
    "\n",
    "Some frequent steps for pre-processing steps are: \n",
    "\n",
    "* Lower casing \n",
    "* Removal of Punctuations \n",
    "* Removal of Stopwords \n",
    "* Removal of Frequent words \n",
    "* Stemming \n",
    "* Lemmatization \n",
    "* Removal of emojis \n",
    "* Removal of emoticons \n",
    "\n",
    " \n",
    "Well these steps are frequent but they may vary problem to problem. For e.g. let us consider we want to predict whether the given text belong to news, music or any other field. So for this problem we cannot remove the Frequent words like news article might contain the word news in it a lot, Hence to categorize the text we cannot just remove it. \n",
    "\n",
    "Now you might be wondering what is lemmatization and stemming are. So before we move forward lets understand these terms. \n",
    "\n",
    "Stemming and Lemmatization helps us to achieve the root forms (sometimes called synonyms in search context) of inflected (derived) words.  \n",
    "\n",
    " \n",
    "\n",
    "###  Stemming : \n",
    "\n",
    "Stemming is faster because it chops words without knowing the context of the words in given sentences. \n",
    "\n",
    "* It is rule-based approach. \n",
    "* Accuracy is less. \n",
    "* When we convert any words into root-form then stemming may create the non-existence meaning of a word. \n",
    "* Stemming is preferred when the meaning of the word is not important for analysis. For e.g in spam detection.\n",
    "* For example : Studies => Studi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eXV2ZRmfdunV",
    "outputId": "dc84308e-e665-4bb9-bb5b-d54e9852eac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: Da  Stem: Da\n",
      "Actual: Vinci  Stem: vinci\n",
      "Actual: Code  Stem: code\n",
      "Actual: is  Stem: is\n",
      "Actual: such  Stem: such\n",
      "Actual: an  Stem: an\n",
      "Actual: amazing  Stem: amaz\n",
      "Actual: book  Stem: book\n",
      "Actual: to  Stem: to\n",
      "Actual: read  Stem: read\n",
      "Actual: .  Stem: .\n",
      "Actual: .The  Stem: .the\n",
      "Actual: book  Stem: book\n",
      "Actual: si  Stem: si\n",
      "Actual: full  Stem: full\n",
      "Actual: of  Stem: of\n",
      "Actual: suspense  Stem: suspens\n",
      "Actual: and  Stem: and\n",
      "Actual: Thriller  Stem: thriller\n",
      "Actual: .  Stem: .\n",
      "Actual: One  Stem: one\n",
      "Actual: of  Stem: of\n",
      "Actual: the  Stem: the\n",
      "Actual: best  Stem: best\n",
      "Actual: work  Stem: work\n",
      "Actual: of  Stem: of\n",
      "Actual: Dan  Stem: dan\n",
      "Actual: Brown  Stem: brown\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "word_data = \"Da Vinci Code is such an amazing book to read.The book is full of suspense and Thriller. One of the best work of Dan Brown.\"\n",
    "# First Word tokenization\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "#Next find the roots of the word\n",
    "for w in nltk_tokens:\n",
    "       print(\"Actual: %s  Stem: %s\"  % (w,porter_stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hM1H5nAfIyU"
   },
   "source": [
    "### Lemmatization :  \n",
    "\n",
    "Lemmatization is slower as compared to stemming but it knows the context of the word before proceeding. \n",
    "\n",
    "* It is a dictionary-based approach. \n",
    "* Accuracy is more as compared to Stemming. \n",
    "* Lemmatization always gives the dictionary meaning word while converting into root-form. \n",
    "* Lemmatization would be recommended when the meaning of the word is important for analysis. for example in Question Answer application.\n",
    "* For Example: “Studies” => “Study” \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnjIsIwF1biK",
    "outputId": "1558119d-efa0-45e8-a419-05ca671d4de3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GS0wkiBfeut5",
    "outputId": "a4e535bc-170d-4d03-8485-bbbda1e568f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: Da  Lemma: Da\n",
      "Actual: Vinci  Lemma: Vinci\n",
      "Actual: Code  Lemma: Code\n",
      "Actual: is  Lemma: is\n",
      "Actual: such  Lemma: such\n",
      "Actual: an  Lemma: an\n",
      "Actual: amazing  Lemma: amazing\n",
      "Actual: book  Lemma: book\n",
      "Actual: to  Lemma: to\n",
      "Actual: read.The  Lemma: read.The\n",
      "Actual: book  Lemma: book\n",
      "Actual: is  Lemma: is\n",
      "Actual: full  Lemma: full\n",
      "Actual: of  Lemma: of\n",
      "Actual: suspense  Lemma: suspense\n",
      "Actual: and  Lemma: and\n",
      "Actual: Thriller  Lemma: Thriller\n",
      "Actual: .  Lemma: .\n",
      "Actual: One  Lemma: One\n",
      "Actual: of  Lemma: of\n",
      "Actual: the  Lemma: the\n",
      "Actual: best  Lemma: best\n",
      "Actual: work  Lemma: work\n",
      "Actual: of  Lemma: of\n",
      "Actual: Dan  Lemma: Dan\n",
      "Actual: Brown  Lemma: Brown\n",
      "Actual: .  Lemma: .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word_data = \"Da Vinci Code is such an amazing book to read.The book is full of suspense and Thriller. One of the best work of Dan Brown.\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "for w in nltk_tokens:\n",
    "       print(\"Actual: %s  Lemma: %s\"  % (w,wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IsRQC5tf7I7"
   },
   "source": [
    "Some other pre-processing steps that are not that common are: \n",
    "\n",
    "### Text Normalization: \n",
    "\n",
    "Text normalization is the process of transforming a text into a canonical (standard) form. For example, the word “gooood” and “gud” can be transformed to “good”, its canonical form. Another example is mapping of near identical words such as “stopwords”, “stop-words” and “stop words” to just “stopwords”. \n",
    "\n",
    " \n",
    "\n",
    "### Language Detection: \n",
    "\n",
    "Well what happen if our text is in other language apart from English. Then our whole pipeline need to be modified according to that language. So for that we need to detect the language before creating the pipeline.  Python provides various modules for language detection.\n",
    "* langdetect\n",
    "* textblob\n",
    "* langrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "ru\n",
      "es\n",
      "no\n",
      "hi\n",
      "ja\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect \n",
    "  \n",
    "# Specifying the language for \n",
    "# detection \n",
    "print(detect(\"Geeksforgeeks is a computer science portal for geeks\")) \n",
    "print(detect(\"Geeksforgeeks - это компьютерный портал для гиков\")) \n",
    "print(detect(\"Geeksforgeeks es un portal informático para geeks\")) \n",
    "print(detect(\"Geeksforgeeks是面向极客的计算机科学门户\")) \n",
    "print(detect(\"Geeksforgeeks geeks के लिए एक कंप्यूटर विज्ञान पोर्टल है\")) \n",
    "print(detect(\"Geeksforgeeksは、ギーク向けのコンピューターサイエンスポータルです。\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Advance Processing  \n",
    "\n",
    "### POS Tagging : \n",
    "Imagine we’re asked to develop a system to identify person and organization names in our company’s collection of one million documents. The common pre-processing steps we discussed earlier may not be relevant in this context. Identifying names requires us to be able to do POS tagging, as identifying proper nouns can be useful in identifying person and organization names. Pre-trained and readily usable POS taggers are implemented in NLP libraries such as NLTK, spaCy and Parsey McParseface Tagger. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l53lffW3gRfT",
    "outputId": "918c5814-1dee-49ea-e5c7-dd7dcca75550"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of speech [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('a', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(\"The quick brown fox jumps over a lazy dog\")\n",
    "print(\"Part of speech\",nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghnMInSdgfbR"
   },
   "source": [
    "### Parse Tree: \n",
    "\n",
    "Now if we have to find the relationship between person and organization then  we need to analyse the sentence in depth and for that  parse tree play a major role. Parse tree is a tree representation of different syntactic categories of a sentence. It helps us to understand the syntactical structure of a sentence. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eDBYCWaX1jEB",
    "outputId": "12e3d9e0-7f2e-496b-fd35-3a6bcbbfa596"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "nltk.download('punkt') \n",
    "nltk.download('averaged_perceptron_tagger') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FFmQ3fr8gkmz",
    "outputId": "23294154-f166-4db6-d65d-4e0b7711fce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Extracting\n",
      " (S\n",
      "  (NP The/DT quick/JJ brown/NN)\n",
      "  (NP fox/NN)\n",
      "  (VP (V jumps/VBZ) (PP (P over/IN) (NP the/DT lazy/JJ dog/NN))))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser \n",
    "   \n",
    "# Example text \n",
    "sample_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "   \n",
    "# Find all parts of speech in above sentence \n",
    "tagged = pos_tag(word_tokenize(sample_text)) \n",
    "   \n",
    "#Extract all parts of speech from any text \n",
    "chunker = RegexpParser(\"\"\" \n",
    "                       NP: {<DT>?<JJ>*<NN>}    #To extract Noun Phrases \n",
    "                       P: {<IN>}               #To extract Prepositions \n",
    "                       V: {<V.*>}              #To extract Verbs \n",
    "                       PP: {<P> <NP>}          #To extract Prepostional Phrases \n",
    "                       VP: {<V> <NP|PP>*}      #To extarct Verb Phrases \n",
    "                       \"\"\") \n",
    "  \n",
    "# Print all parts of speech in above sentence \n",
    "output = chunker.parse(tagged) \n",
    "print(\"After Extracting\\n\", output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_ICPHoeglj-"
   },
   "source": [
    "### Conference Resolution : \n",
    "\n",
    "Coreference resolution is the task of finding all expressions that refer to the same entity in a text. \n",
    "\n",
    "<img src = \"my_icons/topic_02.b.2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRIT6cJHgwVD"
   },
   "source": [
    "# FEATURE ENGINEERING: \n",
    "\n",
    "When we use ML methods to perform our modeling step later, we’ll still need a way to feed this pre-processed text into an ML algorithm. Feature engineering refers to the set of methods that will accomplish this task. It’s also referred to as feature extraction. The goal of feature engineering is to capture the characteristics of the text into a numeric vector that can be understood by the ML algorithms. \n",
    "\n",
    "two different approaches taken in practice for feature engineering in  \n",
    "\n",
    "## classical NLP and traditional ML pipeline  \n",
    "\n",
    "Feature engineering is an integral step in any ML pipeline. Feature engineering steps convert the raw data into a format that can be consumed by a machine. These transformation functions are usually handcrafted in the classical ML pipeline, aligning to the task at hand. For example, imagine a task of sentiment classification on product reviews in e-commerce. One way to convert the reviews into meaningful “numbers” that helps predict the reviews’ sentiments (positive or negative) would be to count the number of positive and negative words in each review. There are statistical measures for understanding if a feature is useful for a task or not. \n",
    "\n",
    "One of the advantages of handcrafted features is that the model remains interpretable—it’s possible to quantify exactly how much each feature is influencing the model prediction. \n",
    "\n",
    " \n",
    "\n",
    "## DL pipeline \n",
    "\n",
    "In the DL pipeline, the raw data (after pre-processing) is directly fed to a model.The model is capable of “learning” features from the data. Hence, these features are more in line with the task at hand, so they generally give improved performance. But, since all these features are learned via model parameters, the model loses interpretability. \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "# RECAP: \n",
    "\n",
    "The first step in the process of developing any NLP system is to collect data relevant to the given task. Even if we’re building a rule-based system, we still need some data to design and test our rules. The data we get is seldom(rarely) clean, and this is where text cleaning comes into play. After cleaning, text data often has a lot of variations and needs to be converted into a canonical (principle or a pre-defined way) form. This is done in the pre-processing step. This is followed by feature engineering, where we carve out indicators that are most suitable for the task at hand.These indicators/features are converted into a format that is understandable by modeling algorithms.  \n",
    "\n",
    "\n",
    ">Note:In the future blog post indepth implementation will be done,so don't worry if things seemed little hazzy:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TnuhFEchBI3"
   },
   "source": [
    "\n",
    "{{ 'Notes are compiled from [ Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/), [GeeksforGeeks](https://www.geeksforgeeks.org/syntax-tree-natural-language-processing/), [tutorialspoint-Stemming and Lemmatization](https://www.tutorialspoint.com/python_data_science/python_stemming_and_lemmatization.htm), [gfg-parse tree](https://www.geeksforgeeks.org/syntax-tree-natural-language-processing/),[GeeksforGeeks-Language detection](https://www.geeksforgeeks.org/detect-an-unknown-language-using-python/), [morioh](https://morioh.com/p/a7b8982e5a5a) and [Medium-Tokenization and Parts of Speech(POS) Tagging in Python’s NLTK library](https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b)' | fndetail: 1 }}\n",
    "{{ 'If you face any problem or have any feedback/suggestions feel free to comment.' | fndetail: 2\n",
    "}}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "c06uVp3rS3wh"
   ],
   "name": "2020-12-25-Building-An-NLP-Pipeline-(PART-2).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
