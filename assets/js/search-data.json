{
  
    
        "post0": {
            "title": "Topic 02.2: Building an NLP Pipeline(PART-2)",
            "content": ". Let&#39;s resume our discussion on NLP pipeline. In the previous blog post we completed the first 2 steps of NLP pipeline that is Data aquisition and text cleaning. In this blog post we will cover data pre-processing and feature engineering. . PRE-PROCESSING: . To pre-process your text simply means to bring your text into a form that is predictable and analysable for your task. A task here is a combination of approach and domain. For example, extracting top keywords with TF-IDF (approach) from Tweets (domain) is an example of a Task. . Task = approach + domain . One task’s ideal pre-processing can become another task’s worst nightmare. So take note: text pre-processing is not directly transferable from task to task. . Let’s take a very simple example, let’s say you are trying to discover commonly used words in a news dataset. If your pre-processing step involves removing stopwords because some other task used it, then you are probably going to miss out on some of the common words as you have ALREADY eliminated it. So really, it’s not a one-size-fits-all approach. . Here are some common pre-processing steps used in NLP software: . Preliminaries: Sentence segmentation and word tokenization. | . Frequent steps: Stop word removal, stemming and lemmatization, removing digits/punctuation, lowercasing, etc. | . Advanced processing: POS tagging, parsing, coreference resolution, etc. | . Preliminaries : . While not all steps will be followed in all the NLP pipelines we encounter, the first two are more or less seen everywhere. Let’s take a look at what each of these steps mean. . The NLP can analysis the text by breaking it into sentence(sentence segmentation) and then further into words(words tokenization). . SENTENCE SEGMENTATION : . We may easily divide the text into sentence on the basis of the position of the full stop(.). But what happen if we have Dr.Joy or (….) in our text. . We have NLP libraries which help to overcome these issue. Like NLTK. . import nltk from nltk.tokenize import sent_tokenize text = &quot;It&#39;s fun to study NLP. Would recommend to all.&quot; print(sent_tokenize(text)) . [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Unzipping tokenizers/punkt.zip. [&#34;It&#39;s fun to study NLP.&#34;, &#39;Would recommend to all.&#39;] . WORD TOKENIZATION : . To tokenize a sentence into words, we can start with a simple rule to split text into words based on the presence of punctuation marks. The NLTK library allows us to do that. . from nltk.tokenize import word_tokenize text = &quot;It&#39;s fun to study NLP. Would recommend to all.&quot; print(word_tokenize(text)) . [&#39;It&#39;, &#34;&#39;s&#34;, &#39;fun&#39;, &#39;to&#39;, &#39;study&#39;, &#39;NLP&#39;, &#39;.&#39;, &#39;Would&#39;, &#39;recommend&#39;, &#39;to&#39;, &#39;all&#39;, &#39;.&#39;] . Frequent Steps : . Some frequent steps for pre-processing steps are: . Lower casing | Removal of Punctuations | Removal of Stopwords | Removal of Frequent words | Stemming | Lemmatization | Removal of emojis | Removal of emoticons | . Well these steps are frequent but they may vary problem to problem. For e.g. let us consider we want to predict whether the given text belong to news, music or any other field. So for this problem we cannot remove the Frequent words like news article might contain the word news in it a lot, Hence to categorize the text we cannot just remove it. . Now you might be wondering what is lemmatization and stemming are. So before we move forward lets understand these terms. . Stemming and Lemmatization helps us to achieve the root forms (sometimes called synonyms in search context) of inflected (derived) words. . Stemming : . Stemming is faster because it chops words without knowing the context of the words in given sentences. . It is rule-based approach. | Accuracy is less. | When we convert any words into root-form then stemming may create the non-existence meaning of a word. | Stemming is preferred when the meaning of the word is not important for analysis. For e.g in spam detection. | For example : Studies =&gt; Studi | . from nltk.stem.porter import PorterStemmer porter_stemmer = PorterStemmer() word_data = &quot;Da Vinci Code is such an amazing book to read.The book is full of suspense and Thriller. One of the best work of Dan Brown.&quot; # First Word tokenization nltk_tokens = nltk.word_tokenize(word_data) #Next find the roots of the word for w in nltk_tokens: print(&quot;Actual: %s Stem: %s&quot; % (w,porter_stemmer.stem(w))) . Actual: Da Stem: Da Actual: Vinci Stem: vinci Actual: Code Stem: code Actual: is Stem: is Actual: such Stem: such Actual: an Stem: an Actual: amazing Stem: amaz Actual: book Stem: book Actual: to Stem: to Actual: read Stem: read Actual: . Stem: . Actual: .The Stem: .the Actual: book Stem: book Actual: si Stem: si Actual: full Stem: full Actual: of Stem: of Actual: suspense Stem: suspens Actual: and Stem: and Actual: Thriller Stem: thriller Actual: . Stem: . Actual: One Stem: one Actual: of Stem: of Actual: the Stem: the Actual: best Stem: best Actual: work Stem: work Actual: of Stem: of Actual: Dan Stem: dan Actual: Brown Stem: brown . Lemmatization : . Lemmatization is slower as compared to stemming but it knows the context of the word before proceeding. . It is a dictionary-based approach. | Accuracy is more as compared to Stemming. | Lemmatization always gives the dictionary meaning word while converting into root-form. | Lemmatization would be recommended when the meaning of the word is important for analysis. for example in Question Answer application. | For Example: “Studies” =&gt; “Study” | . from nltk.stem import WordNetLemmatizer wordnet_lemmatizer = WordNetLemmatizer() word_data = &quot;Da Vinci Code is such an amazing book to read.The book is full of suspense and Thriller. One of the best work of Dan Brown.&quot; nltk_tokens = nltk.word_tokenize(word_data) for w in nltk_tokens: print(&quot;Actual: %s Lemma: %s&quot; % (w,wordnet_lemmatizer.lemmatize(w))) . Actual: Da Lemma: Da Actual: Vinci Lemma: Vinci Actual: Code Lemma: Code Actual: is Lemma: is Actual: such Lemma: such Actual: an Lemma: an Actual: amazing Lemma: amazing Actual: book Lemma: book Actual: to Lemma: to Actual: read.The Lemma: read.The Actual: book Lemma: book Actual: is Lemma: is Actual: full Lemma: full Actual: of Lemma: of Actual: suspense Lemma: suspense Actual: and Lemma: and Actual: Thriller Lemma: Thriller Actual: . Lemma: . Actual: One Lemma: One Actual: of Lemma: of Actual: the Lemma: the Actual: best Lemma: best Actual: work Lemma: work Actual: of Lemma: of Actual: Dan Lemma: Dan Actual: Brown Lemma: Brown Actual: . Lemma: . . Some other pre-processing steps that are not that common are: . Text Normalization: . Text normalization is the process of transforming a text into a canonical (standard) form. For example, the word “gooood” and “gud” can be transformed to “good”, its canonical form. Another example is mapping of near identical words such as “stopwords”, “stop-words” and “stop words” to just “stopwords”. . Language Detection: . Well what happen if our text is in other language apart from English. Then our whole pipeline need to be modified according to that language. So for that we need to detect the language before creating the pipeline. Python provides various modules for language detection. . langdetect | textblob | langrid | . from langdetect import detect # Specifying the language for # detection print(detect(&quot;Geeksforgeeks is a computer science portal for geeks&quot;)) print(detect(&quot;Geeksforgeeks - это компьютерный портал для гиков&quot;)) print(detect(&quot;Geeksforgeeks es un portal informático para geeks&quot;)) print(detect(&quot;Geeksforgeeks是面向极客的计算机科学门户&quot;)) print(detect(&quot;Geeksforgeeks geeks के लिए एक कंप्यूटर विज्ञान पोर्टल है&quot;)) print(detect(&quot;Geeksforgeeksは、ギーク向けのコンピューターサイエンスポータルです。&quot;)) . en ru es no hi ja . Advance Processing . POS Tagging : . Imagine we’re asked to develop a system to identify person and organization names in our company’s collection of one million documents. The common pre-processing steps we discussed earlier may not be relevant in this context. Identifying names requires us to be able to do POS tagging, as identifying proper nouns can be useful in identifying person and organization names. Pre-trained and readily usable POS taggers are implemented in NLP libraries such as NLTK, spaCy and Parsey McParseface Tagger. . tokens = nltk.word_tokenize(&quot;The quick brown fox jumps over a lazy dog&quot;) print(&quot;Part of speech&quot;,nltk.pos_tag(tokens)) . Part of speech [(&#39;The&#39;, &#39;DT&#39;), (&#39;quick&#39;, &#39;JJ&#39;), (&#39;brown&#39;, &#39;NN&#39;), (&#39;fox&#39;, &#39;NN&#39;), (&#39;jumps&#39;, &#39;VBZ&#39;), (&#39;over&#39;, &#39;IN&#39;), (&#39;a&#39;, &#39;DT&#39;), (&#39;lazy&#39;, &#39;JJ&#39;), (&#39;dog&#39;, &#39;NN&#39;)] . Parse Tree: . Now if we have to find the relationship between person and organization then we need to analyse the sentence in depth and for that parse tree play a major role. Parse tree is a tree representation of different syntactic categories of a sentence. It helps us to understand the syntactical structure of a sentence. . from nltk import pos_tag, word_tokenize, RegexpParser # Example text sample_text = &quot;The quick brown fox jumps over the lazy dog&quot; # Find all parts of speech in above sentence tagged = pos_tag(word_tokenize(sample_text)) #Extract all parts of speech from any text chunker = RegexpParser(&quot;&quot;&quot; NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;} #To extract Noun Phrases P: {&lt;IN&gt;} #To extract Prepositions V: {&lt;V.*&gt;} #To extract Verbs PP: {&lt;P&gt; &lt;NP&gt;} #To extract Prepostional Phrases VP: {&lt;V&gt; &lt;NP|PP&gt;*} #To extarct Verb Phrases &quot;&quot;&quot;) # Print all parts of speech in above sentence output = chunker.parse(tagged) print(&quot;After Extracting n&quot;, output) . After Extracting (S (NP The/DT quick/JJ brown/NN) (NP fox/NN) (VP (V jumps/VBZ) (PP (P over/IN) (NP the/DT lazy/JJ dog/NN)))) . Conference Resolution : . Coreference resolution is the task of finding all expressions that refer to the same entity in a text. . . FEATURE ENGINEERING: . When we use ML methods to perform our modeling step later, we’ll still need a way to feed this pre-processed text into an ML algorithm. Feature engineering refers to the set of methods that will accomplish this task. It’s also referred to as feature extraction. The goal of feature engineering is to capture the characteristics of the text into a numeric vector that can be understood by the ML algorithms. . two different approaches taken in practice for feature engineering in . classical NLP and traditional ML pipeline . Feature engineering is an integral step in any ML pipeline. Feature engineering steps convert the raw data into a format that can be consumed by a machine. These transformation functions are usually handcrafted in the classical ML pipeline, aligning to the task at hand. For example, imagine a task of sentiment classification on product reviews in e-commerce. One way to convert the reviews into meaningful “numbers” that helps predict the reviews’ sentiments (positive or negative) would be to count the number of positive and negative words in each review. There are statistical measures for understanding if a feature is useful for a task or not. . One of the advantages of handcrafted features is that the model remains interpretable—it’s possible to quantify exactly how much each feature is influencing the model prediction. . DL pipeline . In the DL pipeline, the raw data (after pre-processing) is directly fed to a model.The model is capable of “learning” features from the data. Hence, these features are more in line with the task at hand, so they generally give improved performance. But, since all these features are learned via model parameters, the model loses interpretability. . RECAP: . The first step in the process of developing any NLP system is to collect data relevant to the given task. Even if we’re building a rule-based system, we still need some data to design and test our rules. The data we get is seldom(rarely) clean, and this is where text cleaning comes into play. After cleaning, text data often has a lot of variations and needs to be converted into a canonical (principle or a pre-defined way) form. This is done in the pre-processing step. This is followed by feature engineering, where we carve out indicators that are most suitable for the task at hand.These indicators/features are converted into a format that is understandable by modeling algorithms. . Note: In the future blog post indepth implementation will be done,so don&#8217;t worry if things seemed little hazzy:) . 1. Notes are compiled from Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems, GeeksforGeeks, tutorialspoint-Stemming and Lemmatization, gfg-parse tree,GeeksforGeeks-Language detection, morioh and Medium-Tokenization and Parts of Speech(POS) Tagging in Python’s NLTK library↩ . 2. If you face any problem or have any feedback/suggestions feel free to comment.↩ .",
            "url": "https://shubhangidabral13.github.io/Bits-and-Bytes-of-NLP/nlp-pipeline/2021/02/01/_12_25_Building_An_NLP_Pipeline_(PART_2).html",
            "relUrl": "/nlp-pipeline/2021/02/01/_12_25_Building_An_NLP_Pipeline_(PART_2).html",
            "date": " • Feb 1, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Topic 02.1: Building an NLP Pipeline(PART-1)",
            "content": "If we were asked to build an NLP application, think about how we would approach doing so at an organization. We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them. Since language processing is involved, we would also list all the forms of text processing needed at each step. . This step-by-step processing of text is known as a NLP pipeline. It is the series of steps involved in building any NLP model. . . The key stages in the pipeline are as follows: . Data acquisition . | Text cleaning . | Pre-processing . | Feature engineering . | Modeling . | Evaluation . | Deployment . | Monitoring and model updating . | Before we dive into NLP applications implementation the first and foremost thing is to get a clear picture about it’s pipeline. Hence, below are a detail overview about each component in it&#39;s pipeline. . Note: The blog post on NLP pipeline is divided into 3 blog post. The first blog post covers the Data acquisition and Text Cleaning. Second blog post covers Pre-processing and Feature engineering and the 3rd blog post covers modeling, Evaluation, Deployment and Moniotring and model updating. . DATA ACQUISITION . Data plays a major role in the NLP pipeline. Hence it&#39;s quite important that how we collect the relevant data for our NLP project. . Sometime it&#39;s easily available to us. But sometime extra effort need to be done to collect these precious data. . 1).Scrape web pages . To create an application that can summarizes the top news into just 100 words .For that you need to scrape the data from the current affairs websites and webpages. . 2).Data Augmentation . NLP has a bunch of techniques through which we can take a small dataset and use some tricks to create more data. These tricks are also called data augmentation, and they try to exploit language properties to create text that is syntactically similar to source text data. They may appear as hacks, but they work very well in practice. Let’s look at some of them: . a).Back translation . Let say we have sentence s1 which is in French. We will translate it to other language (in this case English) and after translation it become sentence s2. Now we will translate this sentence s2 again to French and now it become s3. We’ll find that S1 and S3 are very similar in meaning but there is slight variations. Now we can add S3 to our dataset. . b).Replacing Entities . To create more dataset we will replace the entities name with other entities. Let say s1 is &quot;I want to go to New York&quot;, here we will replace New York with other entity name for e.g. New Jersey. . c).Synonym Replacement . Randomly choose “k” words in a sentence that are not stop words. Replace these words with their synonyms. . d).Bigram flipping . Divide the sentence into bigrams. Take one bigram at random and flip it. For example: “I am going to the supermarket.” Here, we take the bigram “going to” and replace it with the flipped one: “to Going.” . TEXT CLEANING . After collecting data it is also important that data need to be in the form that is understood by computer. Consider the text contains different symbols and words which doesn&#39;t convey meaning to the model while training. So we will remove them before feeding to the model in an efficient way. This method is called Data Cleaning. Different Text Cleaning process are as follows: . HTML tag cleaning . Well when collecting the data we scrap through various web pages. Beautiful Soup and Scrapy, which provide a range of utilities to parse web pages.Hence the text we collect does not have any HTML tag in it. . from bs4 import BeautifulSoup import urllib.request import re url = &quot;https://en.wikipedia.org/wiki/Artificial_intelligence&quot; page = urllib.request.urlopen(url) # connect to website try: page = urllib.request.urlopen(url) except: print(&quot;An error occured.&quot;) soup = BeautifulSoup(page, &#39;html.parser&#39;) regex = re.compile(&#39;^tocsection-&#39;) content_lis = soup.find_all(&#39;li&#39;, attrs={&#39;class&#39;: regex}) content = [] for li in content_lis: content.append(li.getText().split(&#39; n&#39;)[0]) print(content) . [&#39;1 History&#39;, &#39;2 Basics&#39;, &#39;3 Challenges&#39;, &#39;3.1 Reasoning, problem solving&#39;, &#39;3.2 Knowledge representation&#39;, &#39;3.3 Planning&#39;, &#39;3.4 Learning&#39;, &#39;3.5 Natural language processing&#39;, &#39;3.6 Perception&#39;, &#39;3.7 Motion and manipulation&#39;, &#39;3.8 Social intelligence&#39;, &#39;3.9 General intelligence&#39;, &#39;4 Approaches&#39;, &#39;4.1 Cybernetics and brain simulation&#39;, &#39;4.2 Symbolic&#39;, &#39;4.2.1 Cognitive simulation&#39;, &#39;4.2.2 Logic-based&#39;, &#39;4.2.3 Anti-logic or scruffy&#39;, &#39;4.2.4 Knowledge-based&#39;, &#39;4.3 Sub-symbolic&#39;, &#39;4.3.1 Embodied intelligence&#39;, &#39;4.3.2 Computational intelligence and soft computing&#39;, &#39;4.4 Statistical&#39;, &#39;4.5 Integrating the approaches&#39;, &#39;5 Tools&#39;, &#39;6 Applications&#39;, &#39;7 Philosophy and ethics&#39;, &#39;7.1 The limits of artificial general intelligence&#39;, &#39;7.2 Ethical machines&#39;, &#39;7.2.1 Artificial moral agents&#39;, &#39;7.2.2 Machine ethics&#39;, &#39;7.2.3 Malevolent and friendly AI&#39;, &#39;7.3 Machine consciousness, sentience and mind&#39;, &#39;7.3.1 Consciousness&#39;, &#39;7.3.2 Computationalism and functionalism&#39;, &#39;7.3.3 Strong AI hypothesis&#39;, &#39;7.3.4 Robot rights&#39;, &#39;7.4 Superintelligence&#39;, &#39;7.4.1 Technological singularity&#39;, &#39;7.4.2 Transhumanism&#39;, &#39;8 Impact&#39;, &#39;8.1 Risks of narrow AI&#39;, &#39;8.2 Risks of general AI&#39;, &#39;9 Regulation&#39;, &#39;10 In fiction&#39;, &#39;11 See also&#39;, &#39;12 Explanatory notes&#39;, &#39;13 References&#39;, &#39;13.1 AI textbooks&#39;, &#39;13.2 History of AI&#39;, &#39;13.3 Other sources&#39;, &#39;14 Further reading&#39;, &#39;15 External links&#39;] . Unicode Normalization: . While cleaning the data we may also encounter various Unicode characters, including symbols, emojis, and other graphic characters. To parse such non-textual symbols and special characters, we use Unicode normalization. This means that the text we see should be converted into some form of binary representation to store in a computer. This process is known as text encoding. . import emoji text = emoji.emojize(&quot;Python is fun :red_heart:&quot;) print(text) . Python is fun ❤ . Text = text.encode(&quot;utf-8&quot;) print(Text) . b&#39;Python is fun xe2 x9d xa4&#39; . Spelling Correction . The data that we have might have some spelling mistake because of fast typing the text or using short hand or slang that are used on social media like twitter. Using these data may not result in better prediction by our model therefore it is quite important to handle these data before feeding it to the model. we don’t have a robust method to fix this, but we still can make good attempts to mitigate the issue. Microsoft released a REST API that can be used in Python for potential spell checking. . System-Specific Error Correction . What if we need to extract the data from the PDF. Different PDF documents are encoded differently, and sometimes, we may not be able to extract the full text, or the structure of the text may get messed up. There are several libraries, such as PyPDF, PDFMiner, etc., to extract text from PDF documents but they are far from perfect. . | Another common source of textual data is scanned documents. Text extraction from scanned documents is typically done through optical character recognition (OCR), using libraries such as Tesseract. . | . Recap . The first step in the process of developing any NLP system is to collect data relevant to the given task. Even if we’re building a rule-based system, we still need some data to design and test our rules. The data we get is seldom(rarely) clean, and this is where text cleaning comes into play. . 1. Notes are compiled from Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems and morioh↩ . 2. If you face any problem or have any feedback/suggestions feel free to comment.↩ .",
            "url": "https://shubhangidabral13.github.io/Bits-and-Bytes-of-NLP/nlp-pipeline/2021/02/01/_12_24_Building_an_NLP_Pipeline(PART_1).html",
            "relUrl": "/nlp-pipeline/2021/02/01/_12_24_Building_an_NLP_Pipeline(PART_1).html",
            "date": " • Feb 1, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "In the previous topic we have covered the distributional apporach(which have high dimension vector to represent words and also sparse in nature) but in this post we will cover the Distributed apporach(which have low dimension vecotr and are dense in nature) and how to create word embedding using pretrained model. . Distributed Representation . To overcome the issue of high-dimensional representation and sparse vector to represent word, Distributed Representation help in these issue and therefore they have gained a lot of momentum in the past six to seven days. Different distributed representation are . Word Embedding . Word Embeddings are the texts converted into numbers. Embeddings translate large sparse vectors into a lower-dimensional space that preserves semantic relationships. Word embeddings is a technique where individual words of a domain or language are represented as real-valued vectors in a lower dimensional space and placing vectors of semantically similar items close to each other. This way words that have similar meaning have similar distances in the vector space as shown below. . “king is to queen as man is to woman” encoded in the vector space as well as verb Tense and Country and their capitals are encoded in low dimensional space preserving the semantic relationships. . . Word2vec is an algorithm invented at Google for training word embeddings. word2vec relies on the distributional hypothesis. The distributional hypothesis states that words which, often have the same neighboring words tend to be semantically similar. This helps to map semantically similar words to geometrically close embedding vectors. . Now the question arises that how we will create word embedding? . Well we can also use pre-trained word embedding arcitecture or we can also train our own word embedding. . Pre-trained word embeddings . What is pre-trained word embeddings? . Pretrained Word Embeddings are the embeddings learned in one task that are used for solving another similar task. . These embeddings are trained on large datasets, saved, and then used for solving other tasks. That’s why pretrained word embeddings are a form of Transfer Learning. . | . Why do we need Pretrained Word Embeddings? . Pretrained word embeddings capture the semantic and syntactic meaning of a word as they are trained on large datasets. They are capable of boosting the performance of a Natural Language Processing (NLP) model. These word embeddings come in handy during hackathons and of course, in real-world problems as well. . | . But why should we not learn our own embeddings? . Well, learning word embeddings from scratch is a challenging problem due to two primary reasons: . Sparsity of training data | Large number of trainable parameters | . | . With pretrained embedding you just need to download the embeddings and use it to get the vectors for the word you want.Such embeddings can be thought of as a large collection of key-value pairs, where keys are the words in the vocabulary and values are their corresponding word vectors. Some of the most popular pre-trained embeddings are Word2vec by Google, GloVe by Stanford, and fasttext embeddings by Facebook, to name a few. Further, they’re available for various dimensions like d = 25, 50, 100, 200, 300, 600. . Here is the code where we will find the words that are semantically most similar to the word &quot;beautiful&quot;. . #Downdloading Google News vectors embeddings. !wget -P /tmp/input/ -c &quot;https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz&quot; . --2021-02-01 08:38:46-- https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.200.157 Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.200.157|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1647046227 (1.5G) [application/x-gzip] Saving to: ‘/tmp/input/GoogleNews-vectors-negative300.bin.gz’ GoogleNews-vectors- 100%[===================&gt;] 1.53G 46.1MB/s in 35s 2021-02-01 08:39:21 (45.3 MB/s) - ‘/tmp/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227] . from gensim.models import Word2Vec, KeyedVectors pretrainedpath = &#39;/tmp/input/GoogleNews-vectors-negative300.bin.gz&#39; w2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True) #load the model print(&quot;done loading word2vec&quot;) print(&quot;Numver of words in vocablulary: &quot;,len(w2v_model.vocab)) #Number of words in the vocabulary. . done loading word2vec Numver of words in vocablulary: 3000000 . w2v_model.most_similar(&#39;beautiful&#39;) . [(&#39;gorgeous&#39;, 0.8353004455566406), (&#39;lovely&#39;, 0.810693621635437), (&#39;stunningly_beautiful&#39;, 0.7329413890838623), (&#39;breathtakingly_beautiful&#39;, 0.7231341004371643), (&#39;wonderful&#39;, 0.6854087114334106), (&#39;fabulous&#39;, 0.6700063943862915), (&#39;loveliest&#39;, 0.6612576246261597), (&#39;prettiest&#39;, 0.6595001816749573), (&#39;beatiful&#39;, 0.6593326330184937), (&#39;magnificent&#39;, 0.6591402292251587)] . Note that if we search for a word that is not present in the Word2vec model (e.g., “practicalnlp”), we’ll see a “key not found” error. Hence, as a good coding practice, it’s always advised to first check if the word is present in the model’s vocabulary before attempting to retrieve its vector. . w2v_model[&#39;practicalnlp&#39;] . KeyError Traceback (most recent call last) &lt;ipython-input-4-354849ef77a2&gt; in &lt;module&gt;() 1 #What if I am looking for a word that is not in this vocabulary? -&gt; 2 w2v_model[&#39;practicalnlp&#39;] /usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py in __getitem__(self, entities) 335 if isinstance(entities, string_types): 336 # allow calls like trained_model[&#39;office&#39;], as a shorthand for trained_model[[&#39;office&#39;]] --&gt; 337 return self.get_vector(entities) 338 339 return vstack([self.get_vector(entity) for entity in entities]) /usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py in get_vector(self, word) 453 454 def get_vector(self, word): --&gt; 455 return self.word_vec(word) 456 457 def words_closer_than(self, w1, w2): /usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py in word_vec(self, word, use_norm) 450 return result 451 else: --&gt; 452 raise KeyError(&#34;word &#39;%s&#39; not in vocabulary&#34; % word) 453 454 def get_vector(self, word): KeyError: &#34;word &#39;practicalnlp&#39; not in vocabulary&#34; . If you’re new to embeddings, always start by using pre-trained word embeddings in your project. Understand their pros and cons, then start thinking of building your own embeddings. Using pre-trained embeddings will quickly give you a strong baseline for the task at hand. . In the next blog post we will cover the Training our own emdeddings models. . TRAINING OUR OWN EMBEDDINGS . For training our own word embeddings we’ll look at two architectural variants that were propossed in Word2Vec . Continuous bag of words(CBOW) | SkipGram | . Continuous Bag of Words . CBOW tries to learn a language model that tries to predict the “center” word from the words in its context. Let’s understand this using our toy corpus(the quick brown fox jumped over the lazy dog). If we take the word “jumps” as the center word, then its context is formed by words in its vicinity. If we take the context size of 2, then for our example, the context is given by brown, fox, over, the. CBOW uses the context words to predict the target word—jumped. CBOW tries to do this for every word in the corpus; i.e., it takes every word in the corpus as the target word and tries to predict the target word from its corresponding context words. . . Understanding CBOW architecture . . consider the training corpus having the following sentences: . “the dog saw a cat”, “the dog chased the cat”, “the cat climbed a tree” . The corpus vocabulary has eight words. Once ordered alphabetically, each word can be referenced by its index. For this example, our neural network will have eight input neurons and eight output neurons. Let us assume that we decide to use three neurons in the hidden layer. This means that WI and WO will be 8×3 and 3×8 matrices, respectively. Before training begins, these matrices are initialized to small random values as is usual in neural network training. Just for the illustration sake, let us assume WI and WO to be initialized to the following values: . . Suppose we want the network to learn relationship between the words “cat” and “climbed”. That is, the network should show a high probability for “climbed” when “cat” is inputted to the network. In word embedding terminology, the word “cat” is referred as the context word and the word “climbed” is referred as the target word. In this case, the input vector X will be [0 1 0 0 0 0 0 0]. Notice that only the second component of the vector is 1. This is because the input word is “cat” which is holding number two position in sorted list of corpus words. Given that the target word is “climbed”, the target vector will look like [0 0 0 1 0 0 0 0 ]t. . With the input vector representing “cat”, the output at the hidden layer neurons can be computed as . Ht = XtWI = [-0.490796 -0.229903 0.065460] . It should not surprise us that the vector H of hidden neuron outputs mimics the weights of the second row of WI matrix because of 1-out-of-V representation. So the function of the input to hidden layer connections is basically to copy the input word vector to hidden layer. Carrying out similar manipulations for hidden to output layer, the activation vector for output layer neurons can be written as . HtWO = [0.100934 -0.309331 -0.122361 -0.151399 0.143463 -0.051262 -0.079686 0.112928] . now we will use the formula . Thus, the probabilities for eight words in the corpus are: . [0.143073 0.094925 0.114441 0.111166 0.149289 0.122874 0.119431 0.144800] . The probability in bold is for the chosen target word “climbed”. Given the target vector is [0 0 0 1 0 0 0 0 ] . The above description and architecture is meant for learning relationships between pair of words. In the continuous bag of words model, context is represented by multiple words for a given target words. For example, we could use “cat” and “tree” as context words for “climbed” as the target word. This calls for a modification to the neural network architecture. The modification, shown below, consists of replicating the input to hidden layer connections C times, the number of context words, and adding a divide by C operation in the hidden layer neurons. . [An alert reader pointed that the figure below might lead some readers to think that CBOW learning uses several input matrices. It is not so. It is the same matrix, WI, that is receiving multiple input vectors representing different context words] . . I can understand that things can be little hazy at first.But if you read this one more time it will be crystal clear. . In the next blog i will cover skip-gram and other text representation technique. . 1. Notes are compiled from Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems, Medium,CBOW and Skip-gram and Code from github repo↩ . 2. If you face any problem or have any feedback/suggestions feel free to comment.↩ .",
            "url": "https://shubhangidabral13.github.io/Bits-and-Bytes-of-NLP/2021/02/01/_02_1_Text_Representation(PART_2).html",
            "relUrl": "/2021/02/01/_02_1_Text_Representation(PART_2).html",
            "date": " • Feb 1, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Topic 03.1: Text Representation",
            "content": "What is Text Representation? . In NLP the game is all about text. Text that we collect and provide to our computer. . But what if the text we provide cannot be comprehend by our NLP or machine Learning algorithm? Well this nightmare is true because as we know our algorithm cannot handle the text. It might not get the simple English(or any other language text) which we easily grasp. . Number is something that computer always love. So isn&#39;t it&#39;s better that we convert our text into suitable numeric format. . Conversion of raw text to a suitable numerical form is called text representation. . With respect to the larger picture for any NLP problem, the scope of this chapter is depicted by the dotted box in the below figure. . . We will start with simple approaches and go all the way to state-of-the-art techniques for representing text. These approaches are classified into four categories: . Basic vectorization approaches | Distributed representations | Universal language representation | Handcrafted features | . Before starting with Basic Vectorization approaches let me give you a clear picture about Vector space models. . Vector Space Model . Text are represented with a vectors of numbers that are called Vector Space Model(VSM). | VSM is fundamental to many information-retrieval operations, from scoring documents on a query to document classification and document clustering. | It’s a mathematical model that represents text units as vectors. | In the simplest form, these are vectors of identifiers, such as index numbers in a corpus vocabulary. | In this setting, the most common way to calculate similarity between two text blobs is using cosine similarity. . Note: The cosine of the angle between their corresponding vectors. The cosine of 0° is 1 and the cosine of 180° is –1, with the cosine monotonically decreasing from 0° to 180°. Given two vectors, A and B, each with n components, the similarity between them is computed as follows: | . . Basic Vectorization Approaches . To understand different Basic vectorization approaches let’s take a toy corpus with only four documents—D 1 , D 2 , D 3 , D 4 —as an example. . . The vocabulary of this corpus is comprised of six words: [dog, bites, man, eats, meat, food]. We can organize the vocabulary in any order. In this example, we simply take the order in which the words appear in the corpus. . 1.One-Hot Encoding . In one-hot encoding, each word w in the corpus vocabulary is given aunique integer ID w id that is between 1 and |V|, where V is the set of the corpus vocabulary. | Each word is then represented by a V-dimensional binary vector of 0s and 1s. This is done via a |V| dimension vector filled with all 0s except the index, where index = w id . | At this index, we simply put a 1. The representation for individual words is then combined to form a sentence representation. | . Example of toy dataset: . We first map each of the six words to unique IDs: dog = 1, bites = 2, man = 3, meat = 4 , food = 5, eats = 6. | Now for D1 “dog bites man” each word is a six-dimensional vector. | Dog is represented as [1 0 0 0 0 0], as the word “dog” is mapped to ID 1. Bites is represented as [0 1 0 0 0 0], and so on and so forth. Thus, D1 is represented as [ [1 0 0 0 0 0] [0 1 0 0 0 0] [0 0 1 0 0 0]]. | Other documents in the corpus can be represented similarly. | . S1 = &#39;dog bites man&#39; S2 = &#39;man bites dog&#39; S3 = &#39;dog eats meat&#39; S4 = &#39;man eats food&#39; . data = [S1.split(),S2.split(),S3.split(),S4.split()] . from sklearn.preprocessing import OneHotEncoder #One-Hot Encoding onehot_encoder = OneHotEncoder() onehot_encoded = onehot_encoder.fit_transform(data).toarray() print(&quot;Onehot Encoded Matrix: n&quot;,onehot_encoded) . Onehot Encoded Matrix: [[1. 0. 1. 0. 0. 0. 1. 0.] [0. 1. 1. 0. 1. 0. 0. 0.] [1. 0. 0. 1. 0. 0. 0. 1.] [0. 1. 0. 1. 0. 1. 0. 0.]] . Pros of One-Hot Encoding . one-hot encoding is intuitive to understand and straightforward to implement. | . Cons of One-Hot Encoding . The size of the vectors is directly proportional to the size of the vocabulary and also the vectors is sparse. | There is no fixed length representation of text. Text with 10 words is much larger than text with 4 words. | Semantic meaniing is poorly captured by One-Hot Ecnoding. | Cannot handle out of vocabulary(OOV) problem. . Note: OOV is cause when we ecounter the words that are not present in the vocabulary. | . 2.Bag of Words . The key idea behind it is as follows: . represent the text under consideration as a bag (collection) of words while ignoring the order and context. | The basic intuition behind it is that it assumes that the text belonging to a given class in the dataset is characterized by a unique set of words. | If two text pieces have nearly the same words, then they belong to the same bag (class). Thus, by analyzing the words present in a piece of text, one can identify the class (bag) it belongs to. | BoW maps words to unique integer IDs between 1 and |V|. | Each document in the corpus is then converted into a vector of |V| dimensions where in the i th component of the vector, i = w id , is simply the number of times the word w occurs in the document, i.e., we simply score each word in V by their occurrence count in the document. | . Example of toy dataset: . The word IDs are dog = 1, bites = 2, man = 3, meat = 4 , food = 5, eats = 6. | So D1 becomes [1 1 1 0 0 0]. This is because the first three words in the vocabulary appeared exactly once in D1, and the last three did not appear at all. | D4 becomes [0 0 1 0 1 1]. | . documents = [&quot;Dog bites man.&quot;, &quot;Man bites dog.&quot;, &quot;Dog eats meat.&quot;, &quot;Man eats food.&quot;] #Same as the earlier notebook processed_docs = [doc.lower().replace(&quot;.&quot;,&quot;&quot;) for doc in documents] processed_docs . [&#39;dog bites man&#39;, &#39;man bites dog&#39;, &#39;dog eats meat&#39;, &#39;man eats food&#39;] . from sklearn.feature_extraction.text import CountVectorizer #look at the documents list print(&quot;Our corpus: &quot;, processed_docs) count_vect = CountVectorizer() #Build a BOW representation for the corpus bow_rep = count_vect.fit_transform(processed_docs) #Look at the vocabulary mapping print(&quot;Our vocabulary: &quot;, count_vect.vocabulary_) #see the BOW rep for first 2 documents print(&quot;BoW representation for &#39;dog bites man&#39;: &quot;, bow_rep[0].toarray()) print(&quot;BoW representation for &#39;man bites dog: &quot;,bow_rep[1].toarray()) #Get the representation using this vocabulary, for a new text temp = count_vect.transform([&quot;dog and dog are friends&quot;]) print(&quot;Bow representation for &#39;dog and dog are friends&#39;:&quot;, temp.toarray()) . Our corpus: [&#39;dog bites man&#39;, &#39;man bites dog&#39;, &#39;dog eats meat&#39;, &#39;man eats food&#39;] Our vocabulary: {&#39;dog&#39;: 1, &#39;bites&#39;: 0, &#39;man&#39;: 4, &#39;eats&#39;: 2, &#39;meat&#39;: 5, &#39;food&#39;: 3} BoW representation for &#39;dog bites man&#39;: [[1 1 0 0 1 0]] BoW representation for &#39;man bites dog: [[1 1 0 0 1 0]] Bow representation for &#39;dog and dog are friends&#39;: [[0 2 0 0 0 0]] . ~In the above code, we represented the text considering the frequency of words into account. However, sometimes, we don&#39;t care about frequency much, but only want to know whether a word appeared in a text or not. That is, each document is represented as a vector of 0s and 1s. We will use the option binary=True in CountVectorizer for this purpose. . count_vect = CountVectorizer(binary=True) bow_rep_bin = count_vect.fit_transform(processed_docs) temp = count_vect.transform([&quot;dog and dog are friends&quot;]) print(&quot;Bow representation for &#39;dog and dog are friends&#39;:&quot;, temp.toarray()) . Bow representation for &#39;dog and dog are friends&#39;: [[0 1 0 0 0 0]] . Pros of Bag of Words . BoW is fairly simple to understand and implement. | The text have similar length. | documents having the same words will have their vector representations closer to each other as compared to documents with completely different words.BoW scheme captures the semantic similarity of documents. So if two documents have similar vocabulary, they’ll be closer to each other in the vector space and vice versa. | . Cons of Bag of Words . The size of the vectors is directly proportional to the size of the vocabulary and also the vectors is sparse. | It does not capture the similarity between different words that mean the same thing. Say we have three documents: “I run”, “I ran”, and “I ate”. BoW vectors of all three documents will be equally apart. | Cannot handle out of vocabulary(OOV) problem. | In Bag of Words, words order are lost. | . 3.Bag of N-Grams . The bag-of-n-grams (BoN) approach tries to remedy the notion of phrases or word ordering. | This can help us capture some context, which earlier approaches could not do. Each chunk is called an n-gram. | The corpus vocabulary, V, is then nothing but a collection of all unique n-grams across the text corpus. | Then, each document in the corpus is represented by a vector of length |V|. | This vector simply contains the frequency counts of n-grams present in the document and zero for the n-grams that are not present. | . Example of toy dataset: . Let’s construct a 2- gram (a.k.a. bigram) model for it. | The set of all bigrams in the corpus is as follows: {dog bites, bites man, man bites, bites dog, dog eats, eats meat, man eats, eats food}. | Then, BoN representation consists of an eight-dimensional vector for each document. | The bigram representation for the first two documents is as follows: D 1 : [1,1,0,0,0,0,0,0], D 2 : [0,0,1,1,0,0,0,0]. | The other two documents follow similarly. | . ~CountVectorizer, which we used for BoW, can be used for getting a Bag of N-grams representation as well, using its ngram_range argument. . from sklearn.feature_extraction.text import CountVectorizer #Ngram vectorization example with count vectorizer and uni, bi, trigrams count_vect = CountVectorizer(ngram_range=(1,3)) #Build a BOW representation for the corpus bow_rep = count_vect.fit_transform(processed_docs) #Look at the vocabulary mapping print(&quot;Our vocabulary: &quot;, count_vect.vocabulary_) #see the BOW rep for first 2 documents print(&quot;BoW representation for &#39;dog bites man&#39;: &quot;, bow_rep[0].toarray()) print(&quot;BoW representation for &#39;man bites dog: &quot;,bow_rep[1].toarray()) #Get the representation using this vocabulary, for a new text temp = count_vect.transform([&quot;dog and dog are friends&quot;]) print(&quot;Bow representation for &#39;dog and dog are friends&#39;:&quot;, temp.toarray()) . Our vocabulary: {&#39;dog&#39;: 3, &#39;bites&#39;: 0, &#39;man&#39;: 12, &#39;dog bites&#39;: 4, &#39;bites man&#39;: 2, &#39;dog bites man&#39;: 5, &#39;man bites&#39;: 13, &#39;bites dog&#39;: 1, &#39;man bites dog&#39;: 14, &#39;eats&#39;: 8, &#39;meat&#39;: 17, &#39;dog eats&#39;: 6, &#39;eats meat&#39;: 10, &#39;dog eats meat&#39;: 7, &#39;food&#39;: 11, &#39;man eats&#39;: 15, &#39;eats food&#39;: 9, &#39;man eats food&#39;: 16} BoW representation for &#39;dog bites man&#39;: [[1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0]] BoW representation for &#39;man bites dog: [[1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0]] Bow representation for &#39;dog and dog are friends&#39;: [[0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] . Pros of Bag of N-Gram . It captures some context and word-order information in the form of n-grams. | Thus, resulting vector space is able to capture some semantic similarity. | . Cons of Bag of N-Gram . As n increases, dimensionality (and therefore sparsity) only increases rapidly. | It still provides no way to address the OOV problem. | . 4.TF-IDF . TF-IDF, or term frequency–inverse document frequency assign importance to the word in the documents. . The intuition behind TF-IDF is as follows: . if a word w appears many times in a document d i but does not occur much in the rest of the documents dj in the corpus, then the word w must be of great importance to the document di. | The importance of w should increase in proportion to its frequency in di , but at the same time, its importance should decrease in proportion to the word’s frequency in other documents dj in the corpus. | Mathematically, this is captured using two quantities: TF and IDF. The two are then combined to arrive at the TF-IDF score. | . TF (term frequency) . measures how often a term or word occurs in a given document. | Since different documents in the corpus may be ofdifferent lengths, a term may occur more often in a longer document as compared to a shorter document. | To normalize these counts, we divide the number of occurrences by the length of the document. | . TF of a term t in a document d is defined as: . . DF (inverse document frequency) . measures the importance of the term across a corpus. | In computing TF, all terms are given equal importance (weightage). | However, it’s a well-known fact that stop words like is, are, am, etc., are not important, even though they occur frequently. | To account for such cases, IDF weighs down the terms that are very common across a corpus and weighs up the rare terms. | . IDF of a term t is calculated as follows: . . Thus, TF-IDF score = TF * IDF. . from sklearn.feature_extraction.text import TfidfVectorizer tfidf = TfidfVectorizer() bow_rep_tfidf = tfidf.fit_transform(processed_docs) #IDF for all words in the vocabulary print(&quot;IDF for all words in the vocabulary&quot;,tfidf.idf_) print(&quot;-&quot;*10) #All words in the vocabulary. print(&quot;All words in the vocabulary&quot;,tfidf.get_feature_names()) print(&quot;-&quot;*10) #TFIDF representation for all documents in our corpus print(&quot;TFIDF representation for all documents in our corpus n&quot;,bow_rep_tfidf.toarray()) print(&quot;-&quot;*10) temp = tfidf.transform([&quot;dog and man are friends&quot;]) print(&quot;Tfidf representation for &#39;dog and man are friends&#39;: n&quot;, temp.toarray()) . IDF for all words in the vocabulary [1.51082562 1.22314355 1.51082562 1.91629073 1.22314355 1.91629073] - All words in the vocabulary [&#39;bites&#39;, &#39;dog&#39;, &#39;eats&#39;, &#39;food&#39;, &#39;man&#39;, &#39;meat&#39;] - TFIDF representation for all documents in our corpus [[0.65782931 0.53256952 0. 0. 0.53256952 0. ] [0.65782931 0.53256952 0. 0. 0.53256952 0. ] [0. 0.44809973 0.55349232 0. 0. 0.70203482] [0. 0. 0.55349232 0.70203482 0.44809973 0. ]] - Tfidf representation for &#39;dog and man are friends&#39;: [[0. 0.70710678 0. 0. 0.70710678 0. ]] . Pros of TF-IDF . Assign importance to the words in a particular documents. | . Cons of TF-IDF . The feature vectors are sparse and high-dimensional representations. | They cannot handle OOV problem. | . Well that now we have completed all the basic vectorization approach. In the next topic we will cover other approaches for text representation followed by state-of-art-approach. .",
            "url": "https://shubhangidabral13.github.io/Bits-and-Bytes-of-NLP/basic-nlp/2021/02/01/_01_20_Text_Representation(PART-1).html",
            "relUrl": "/basic-nlp/2021/02/01/_01_20_Text_Representation(PART-1).html",
            "date": " • Feb 1, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Topic 02.3: Building an NLP Pipeline(PART-3)",
            "content": ". Let&#39;s resume our discussion on NLP pipeline. In the previous 2 blog post we completed the first 4 steps of NLP pipeline. In this blog post we will cover Modeling, Evaluation, Deployement and Monitoring and Model Updating. . Modeling . The next step is about how to build a useful solution out of this. At the start, when we have limited data, we can use simpler methods and rules. Over time, with more data and a better understanding of the problem, we can add more complexity and improve performance.There are different ways to create the model. They are as follows:- . Start with Simple Heuristics . At the very start of building a model, ML may not play a major role by itself. Part of that could be due to a lack of data, but human-built heuristics can also provide a great start in some ways.For instance, in email spam-classification tasks, we may have a blacklist of domains that are used exclusively to send spam. This information can be used to filter emails from those domains. Similarly, a blacklist of words in an email that denote a high chance of spam could also be used for this classification. . Another popular approach to incorporating heuristics in your system is using regular expression. Stanford NLP’s TokensRegex and spaCy’s rule-based matching are two tools that are useful for defining advanced regular expressions to capture other information. . Building Your Model . While a set of simple heuristics is a good start, as our system matures,adding newer and newer heuristics may result in a complex, rule-based system. Such a system is hard to manage, and it can be even harder to diagnose the cause of errors. We need a system that’s easier to maintain as it matures. Further, as we collect more data, our ML model starts beating pure heuristics. At that point, a common practice is to combine heuristics directly or indirectly with the ML model.There are two broad ways of doing that: . a). Create a feature from the heuristic for your ML model . When there are many heuristics where the behavior of a single heuristic is deterministic but their combined behavior is fuzzy in terms of how they predict, it’s best to use these heuristics as features to train your ML model. For instance, in the email spam-classification example, we can add features, such as the number of words from the blacklist in a given email or the email bounce rate, to the ML model. . b).Pre-process your input to the ML model . If the heuristic has a really high prediction for a particular kind of class, then it’s best to use it before feeding the data in your ML model. For instance, if for certain words in an email, there’s a 99% chance that it’s spam, then it’s best to classify that email as spam instead of sending it to an ML model. . Building The Model . After following the baseline of modeling starting from simple Heuristics followed by Building your model the final step is to build &quot;THE MODEL&quot; that gives good performance and is also production-ready. For that we may have to do many iterations of the model-building process.We cover some of the approaches to address this issue here: . 1.Ensemble and stacking . a common practice is not to have a single model, but to use a collection of ML models, often dealing with different aspects of the prediction problem. There are two ways of doing this: we can feed one model’s output as input for another model, thus sequentially going from one model to another and obtaining a final output. This is called model stacking. Alternatively, we can also pool predictions from multiple models and make a final prediction. This is called model ensembling. . 2.Better feature engineering . A better feature engineering step may lead to better performance. For instance, if there are a lot of features, then we use feature selection to find a better model. Detail about feature engineering will be coverd in future topic. . 3.Transfer learning . Transfer learning is a machine learning technique where a model trained on one task is re-purposed on a second related task. As an example, for email spam classification, we can use BERT to fine-tune the email dataset. . 4.Reapplying heuristics . No ML model is perfect. Hence, ML models still make mistakes.It’s possible to revisit these cases again at the end of the modeling pipeline to find any common pattern in errors and use heuristicsto correct them. We can also apply domain-specific knowledge that is not automatically captured in the data to refine the model predictions. . Evaluation . . A key step in the NLP pipeline is to measure how good the model we’ve built is.Success in this phase depends on two factors: . using the right metric for evaluation | following the right evaluation process. | . evaluations are of two types: intrinsic and extrinsic. Intrinsic focuses on intermediary objectives, while extrinsic focuses on evaluating performance on the final objective. For example, consider a spam-classification system. The ML metric will be precision and recall, while the business metric will be “the amount of time users spent on a spam email.” Intrinsic evaluation will focus on measuring the system performance using precision and recall. Extrinsic evaluation will focus on measuring the time a user wasted because a spam email went to their inbox or a genuine email went to their spam folder. . Deployment . An NLP model can only begin to add value to an organization when that model’s insights routinely become available to the users for which it was built. The process of taking a trained model and making its predictions available to users or other systems is known as deployment. Various cloud services like AWS,Azure make deployement quite easy and effective. More detials will be covered in the future tutorials. . Monitoring and Model Updating . Monitoring . the model performance is monitored constantly after deploying. Monitoring for NLP projects and models has to be handled differently than a regular engineering project, as we need to ensure that the outputs produced by our models daily make sense. If we’re automatically training the model frequently, we have to make sure that the models behave in a reasonable manner. . Model Updating . Once the model is deployed and we start gathering new data, we’ll iterate the model based on this new data to stay current with predictions. . Detialed and practical implementation will be covered as we will move forward with the topics. .",
            "url": "https://shubhangidabral13.github.io/Bits-and-Bytes-of-NLP/nlp-pipeline/2021/02/01/_01_02_Building_An_NLP_Pipeline_(PART_3).html",
            "relUrl": "/nlp-pipeline/2021/02/01/_01_02_Building_An_NLP_Pipeline_(PART_3).html",
            "date": " • Feb 1, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Topic 01: Introduction to Natural Language Proccessing",
            "content": ". What Is NLP? . Everything we express (either verbally or in written) carries a huge amount of information. The topic we choose, our tone, our selection of words, everything adds some type of information that can be interpreted and value can be extracted from it. In theory, we can understand and even predict human behaviour using that information. . Even for centuries a large amount of data has been stored in books, manuscripts and as data is a boon in(IT sector) today&#39;s world so using this data may act as a helping hand for making our technology more innovative. . Well, it&#39;s not that as simple as it seems. There is some issue while handling these types of data. . Data is unstructured: Data generated from conversations, declarations, or even tweets are examples of unstructured data. Unstructured data doesn’t fit neatly into the traditional row and column structure of relational databases, and represent the vast majority of data available in the actual world. It is messy and hard to manipulate. . A computer only understands numeric data: that data generated from books, speech, text, etc are not in numeric form hence it&#39;s difficult for a computer to interpret these data. . Here where Natural Language Processing comes to the rescue. . Natural Language is a language that we humans can understand e.g. Hindi, English, Spanish, etc and these languages are not understood by computer hence for a computer to understand these languages they need to process it and convert it into the form which the computer can interpret so the term Natural Language Processing is coiled. . . How NLP is applicable in today&#39;s industries? . There is a collection of fundamental tasks that appear frequently across various NLP projects. . 1).Language modeling . This is the task of predicting what the next word in a sentence will be based on the history of previous words. The goal of this task is to learn the probability of a sequence of words appearing in a given language. Language modeling is useful for buildings solutions for a wide variety of problems, such as speech recognition, optical character recognition, handwriting recognition, machine translation, and spelling correction. . 2).Text classification . This is the task of bucketing the text into a known set of categories based on its content. Text classification is by far the most popular task in NLP and is used in a variety of tools, from email spam identification to sentiment analysis. . 3).Information extraction . As the name indicates, this is the task of extracting relevant information from text, such as calendar events from emails or the names of people mentioned in a social media post. . 4).Information retrieval . This is the task of finding documents relevant to a user query from a large collection. Applications like Google Search are well-known use cases of information retrieval. . 5).Conversational agent . This is the task of building dialogue systems that can converse in human languages. Alexa, Siri, etc., are some common applications of this task. . 6).Text summarization . This task aims to create short summaries of longer documents while retaining the core content and preserving the overall meaning of the text. . 7).Question answering . This is the task of building a system that can automatically answer questions posed in natural language. . 8).Machine translation . This is the task of converting a piece of text from one language to another. Tools like Google Translate are common applications of this task. . 9).Topic modeling . This is the task of uncovering the topical structure of a large collection of documents. Topic modeling is a common text mining tool and is used in a wide range of domains, from literature to bioinformatics. . Approaches to NLP . The different approaches used to solve NLP problems commonly fall into three categories: heuristics, machine learning, and deep learning. . Heuristic-Based NLP . Similar to other early AI systems, early attempts at designing NLP systems were based on building rules for the task at hand. This required that the developers had some expertise in the domain to formulate rules that could be incorporated into a program. Such systems also required resources like dictionaries and thesauruses, typically compiled and digitized over a period of time. . Advantage : . It&#39;s easily adaptable . | Simple to debug . | Enormous training corpus not needed . | Comprehends the language . | High perfection . | . | . Disadvantage : . Proficient developers and linguists required . | Slow parser development . | Moderate recall (coverage) . | . | . Machine Learning in NLP . Machine learning techniques are applied to textual data just as they’re used on other forms of data, such as images, speech, and structure data. Supervised machine learning techniques such as classification and regression methods are heavily used for various NLP tasks. As an example, an NLP classification task would be to classify news articles into a set of news topics like sports or politics. On the other hand, regression techniques, which give a numeric prediction, can be used to estimate the price of a stock based on processing the social media discussion about that stock. Similarly, unsupervised clustering algorithms can be used to club together text documents. . Advantage : . It&#39;s can scale effortlessly . | Learnability without clear programming. . | Quick development if the dataset is available. . | . | . Disadvantage : . Training corpus with annotation needed . | Hard to debug . | Zero understanding of the language. . | . | . Deep Learning for NLP . In the last few years, we have seen a huge surge in using neural networks to deal with complex, unstructured data. Language is inherently complex and unstructured. Therefore, we need models with better representation and learning capability to understand and solve language tasks. Here are a few popular deep neural network architectures that have become the status quo in NLP. . Recurrent Neural Networks . | Long-short Term Memory . | Convolutional Neural Networks . | Transformers . | Autoencoders . | . With this broad overview in place, let’s start delving deeper into the world of NLP. . 1. Notes are compiled from Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems, Introduction to Natural Language Processing (NLP)↩ . 2. If you face any problem or have any feedback/suggestions feel free to comment.↩ .",
            "url": "https://shubhangidabral13.github.io/Bits-and-Bytes-of-NLP/basic-nlp/2020/12/22/Introduction-To-Natural-Language-Processing.html",
            "relUrl": "/basic-nlp/2020/12/22/Introduction-To-Natural-Language-Processing.html",
            "date": " • Dec 22, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "",
          "url": "https://shubhangidabral13.github.io/Bits-and-Bytes-of-NLP/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://shubhangidabral13.github.io/Bits-and-Bytes-of-NLP/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}