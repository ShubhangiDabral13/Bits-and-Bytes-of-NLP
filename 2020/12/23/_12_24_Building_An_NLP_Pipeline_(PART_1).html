<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Topic 2.1: Building an NLP Pipeline | Bits and Bytes of NLP</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Topic 2.1: Building an NLP Pipeline" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A technical blog which cover all Bits and Bytes of NLP." />
<meta property="og:description" content="A technical blog which cover all Bits and Bytes of NLP." />
<link rel="canonical" href="https://shubhangidabral13.github.io/Bits-and-Bytes-of-NLP/2020/12/23/_12_24_Building_An_NLP_Pipeline_(PART_1).html" />
<meta property="og:url" content="https://shubhangidabral13.github.io/Bits-and-Bytes-of-NLP/2020/12/23/_12_24_Building_An_NLP_Pipeline_(PART_1).html" />
<meta property="og:site_name" content="Bits and Bytes of NLP" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-23T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://shubhangidabral13.github.io/Bits-and-Bytes-of-NLP/2020/12/23/_12_24_Building_An_NLP_Pipeline_(PART_1).html","@type":"BlogPosting","headline":"Topic 2.1: Building an NLP Pipeline","dateModified":"2020-12-23T00:00:00-06:00","datePublished":"2020-12-23T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://shubhangidabral13.github.io/Bits-and-Bytes-of-NLP/2020/12/23/_12_24_Building_An_NLP_Pipeline_(PART_1).html"},"description":"A technical blog which cover all Bits and Bytes of NLP.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/Bits-and-Bytes-of-NLP/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://shubhangidabral13.github.io/Bits-and-Bytes-of-NLP/feed.xml" title="Bits and Bytes of NLP" /><link rel="shortcut icon" type="image/x-icon" href="/Bits-and-Bytes-of-NLP/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" /><script src="https://hypothes.is/embed.js" async></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/Bits-and-Bytes-of-NLP/">Bits and Bytes of NLP</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/Bits-and-Bytes-of-NLP/about/">About Me</a><a class="page-link" href="/Bits-and-Bytes-of-NLP/search/">Search</a><a class="page-link" href="/Bits-and-Bytes-of-NLP/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Topic 2.1: Building an NLP Pipeline</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-12-23T00:00:00-06:00" itemprop="datePublished">
        Dec 23, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      16 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/ShubhangiDabral13/Bits-and-Bytes-of-NLP/tree/master/_notebooks/2020_12_24_Building_An_NLP_Pipeline_(PART_1).ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/Bits-and-Bytes-of-NLP/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/ShubhangiDabral13/Bits-and-Bytes-of-NLP/master?filepath=_notebooks%2F2020_12_24_Building_An_NLP_Pipeline_%28PART_1%29.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/Bits-and-Bytes-of-NLP/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/ShubhangiDabral13/Bits-and-Bytes-of-NLP/blob/master/_notebooks/2020_12_24_Building_An_NLP_Pipeline_(PART_1).ipynb" target="_blank">
        <img class="notebook-badge-image" src="/Bits-and-Bytes-of-NLP/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020_12_24_Building_An_NLP_Pipeline_(PART_1).ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we were asked to build an NLP application, think about how we would approach doing so at an organization. We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them. Since language processing is involved, we would also list all the forms of text processing needed at each step.</p>
<p><em>This step-by-step processing of text is known as a NLP pipeline. It is the series of steps involved in building any NLP model.</em></p>
<p><figure>
  
    <img class="docimage" src="/Bits-and-Bytes-of-NLP/images/copied_from_nb/my_icons/topic_02.a.1.png" alt="" />
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The key stages in the pipeline are as follows:</p>
<ol>
<li><p>Data acquisition</p>
</li>
<li><p>Text cleaning</p>
</li>
<li><p>Pre-processing</p>
</li>
<li><p>Feature engineering</p>
</li>
<li><p>Modeling</p>
</li>
<li><p>Evaluation</p>
</li>
<li><p>Deployment</p>
</li>
<li><p>Monitoring and model updating</p>
</li>
</ol>
<p>Before we dive into NLP applications implementation the first and foremost thing is to get a clear picture about it’s pipeline. Hence, below are a detail overview about each component in it's pipeline.</p>
<p>NOTE: This blog post is divided into 2 parts. First part will deal with data with data collection, cleaning,  pre-processing and feature engineering and the second part deal with model building , evaluation , deployment and monitoring and updating the model.</p>
<h1 id="DATA-ACUQISTION">DATA ACUQISTION<a class="anchor-link" href="#DATA-ACUQISTION"> </a></h1><p>Data plays a major role in the NLP pipeline. Hence it's quite important that how we collect the relevant data for our NLP project.</p>
<p>Sometime it's easily available to us. But sometime extra effort need to be done to collect  these precious data.</p>
<h2 id="1).Scrape-web-pages:">1).Scrape web pages:<a class="anchor-link" href="#1).Scrape-web-pages:"> </a></h2><p>To create an application that can summarizes the top news into just 100 words . So for that you need to scrape the data from the current affairs websites and webpages.</p>
<h2 id="2).Data-Augmentation:">2).Data Augmentation:<a class="anchor-link" href="#2).Data-Augmentation:"> </a></h2><p>NLP has a bunch of techniques through which we can take a small dataset and use some tricks to create more data. These tricks are also called data augmentation, and they try to exploit language properties to create text that is syntactically similar to source text data. They may appear as hacks, but they work very well in practice. Let’s look at some of them:</p>
<h2 id="3).Back-translation-:">3).Back translation :<a class="anchor-link" href="#3).Back-translation-:"> </a></h2><p>Let say we have sentence s1 which is in French. We will translate it to other language (in this case English) and after translation it become sentence s2. Now we will translate this sentence s2 to again French and now it become s3. We’ll find that S1 and S3 are very similar in meaning but are slight variations of each other. Now we can add S3 to our dataset.</p>
<h2 id="4).Replacing-Entities:">4).Replacing Entities:<a class="anchor-link" href="#4).Replacing-Entities:"> </a></h2><p>To create more dataset we will replace the entities name with other entities. Let say s1 is "I want to go to New York", here we will replace New York with other entity name for e.g. New Jersey.</p>
<h2 id="5).Synonym-Replacement:">5).Synonym Replacement:<a class="anchor-link" href="#5).Synonym-Replacement:"> </a></h2><p>Randomly choose “k” words in a sentence that are not stop words. Replace these words with their synonyms.</p>
<h2 id="6).Bigram-flipping:">6).Bigram flipping:<a class="anchor-link" href="#6).Bigram-flipping:"> </a></h2><p>Divide the sentence into bigrams. Take one bigram at random and flip it. For example: “I am going to the supermarket.” Here, we take the bigram “going to” and replace it with the flipped one: “to Going.”</p>
<h1 id="TEXT-CLEANING">TEXT CLEANING<a class="anchor-link" href="#TEXT-CLEANING"> </a></h1><p>After  collecting data it is also important that data need to be in the form that is understood by computer. Consider the text contains different symbols and words which doesn't convey meaning to the model while training. So we will remove them before feeding to the model in an efficient way. This method is called Data Cleaning. Different Text Cleaning process are as follows:</p>
<h2 id="HTML-tag-cleaning-:">HTML tag cleaning :<a class="anchor-link" href="#HTML-tag-cleaning-:"> </a></h2><p>Well when collecting the data we scrap through various web pages. Beautiful Soup and Scrapy, which provide a range of utilities to parse web pages.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>
<span class="kn">import</span> <span class="nn">urllib.request</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://en.wikipedia.org/wiki/Artificial_intelligence&quot;</span>
<span class="n">page</span> <span class="o">=</span> <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlopen</span><span class="p">(</span><span class="n">url</span><span class="p">)</span> <span class="c1"># conntect to website</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">page</span> <span class="o">=</span> <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlopen</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;An error occured.&quot;</span><span class="p">)</span>
<span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">page</span><span class="p">,</span> <span class="s1">&#39;html.parser&#39;</span><span class="p">)</span>
<span class="n">regex</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s1">&#39;^tocsection-&#39;</span><span class="p">)</span>
<span class="n">content_lis</span> <span class="o">=</span> <span class="n">soup</span><span class="o">.</span><span class="n">find_all</span><span class="p">(</span><span class="s1">&#39;li&#39;</span><span class="p">,</span> <span class="n">attrs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;class&#39;</span><span class="p">:</span> <span class="n">regex</span><span class="p">})</span>
<span class="n">content</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">li</span> <span class="ow">in</span> <span class="n">content_lis</span><span class="p">:</span>
    <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">li</span><span class="o">.</span><span class="n">getText</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;1 History&#39;, &#39;2 Basics&#39;, &#39;3 Challenges&#39;, &#39;3.1 Reasoning, problem solving&#39;, &#39;3.2 Knowledge representation&#39;, &#39;3.3 Planning&#39;, &#39;3.4 Learning&#39;, &#39;3.5 Natural language processing&#39;, &#39;3.6 Perception&#39;, &#39;3.7 Motion and manipulation&#39;, &#39;3.8 Social intelligence&#39;, &#39;3.9 General intelligence&#39;, &#39;4 Approaches&#39;, &#39;4.1 Cybernetics and brain simulation&#39;, &#39;4.2 Symbolic&#39;, &#39;4.2.1 Cognitive simulation&#39;, &#39;4.2.2 Logic-based&#39;, &#39;4.2.3 Anti-logic or scruffy&#39;, &#39;4.2.4 Knowledge-based&#39;, &#39;4.3 Sub-symbolic&#39;, &#39;4.3.1 Embodied intelligence&#39;, &#39;4.3.2 Computational intelligence and soft computing&#39;, &#39;4.4 Statistical&#39;, &#39;4.5 Integrating the approaches&#39;, &#39;5 Tools&#39;, &#39;6 Applications&#39;, &#39;7 Philosophy and ethics&#39;, &#39;7.1 The limits of artificial general intelligence&#39;, &#39;7.2 Ethical machines&#39;, &#39;7.2.1 Artificial moral agents&#39;, &#39;7.2.2 Machine ethics&#39;, &#39;7.2.3 Malevolent and friendly AI&#39;, &#39;7.3 Machine consciousness, sentience and mind&#39;, &#39;7.3.1 Consciousness&#39;, &#39;7.3.2 Computationalism and functionalism&#39;, &#39;7.3.3 Strong AI hypothesis&#39;, &#39;7.3.4 Robot rights&#39;, &#39;7.4 Superintelligence&#39;, &#39;7.4.1 Technological singularity&#39;, &#39;7.4.2 Transhumanism&#39;, &#39;8 Impact&#39;, &#39;8.1 Risks of narrow AI&#39;, &#39;8.2 Risks of general AI&#39;, &#39;9 Regulation&#39;, &#39;10 In fiction&#39;, &#39;11 See also&#39;, &#39;12 Explanatory notes&#39;, &#39;13 References&#39;, &#39;13.1 AI textbooks&#39;, &#39;13.2 History of AI&#39;, &#39;13.3 Other sources&#39;, &#39;14 Further reading&#39;, &#39;15 External links&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Unicode-Normalization:">Unicode Normalization:<a class="anchor-link" href="#Unicode-Normalization:"> </a></h2><p>While cleaning the data we may also encounter various Unicode characters, including symbols, emojis, and other graphic characters. To parse such non-textual symbols and special characters, we use Unicode normalization. This means that the text we see should be converted into some form of binary representation to store in a computer. This process is known as text encoding.</p>
<INSERT CODE="" HERE=""> 
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">emoji</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">emoji</span><span class="o">.</span><span class="n">emojize</span><span class="p">(</span><span class="s2">&quot;Python is fun :red_heart:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Python is fun ❤
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Text</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>b&#39;Python is fun \xe2\x9d\xa4&#39;
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Spelling-Correction:">Spelling Correction:<a class="anchor-link" href="#Spelling-Correction:"> </a></h2><p>The data that we have might have some spelling mistake because of fast typing the text or using short hand or slang that are used on social media like twitter. Using these data may not result in better prediction by our model therefore it is quite important to handle these data before feeding it to the model. we don’t have a robust method to fix this, but we still can make good attempts to mitigate the issue. Microsoft released a REST API that can be used in Python for potential spell checking.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="System-Specific-Error-Correction:">System-Specific Error Correction:<a class="anchor-link" href="#System-Specific-Error-Correction:"> </a></h3><ul>
<li><p>What if we need to extract the data from the PDF. Different PDF documents are encoded differently, and sometimes, we may not be able to extract the full text, or the structure of the text may get messed up. There are several libraries, such as PyPDF, PDFMiner, etc., to extract text from PDF documents but they are far from perfect.</p>
</li>
<li><p>Another common source of textual data is scanned documents. Text extraction from scanned documents is typically done through optical character recognition (OCR), using libraries such as Tesseract.</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="PRE-PROCESSING:">PRE-PROCESSING:<a class="anchor-link" href="#PRE-PROCESSING:"> </a></h1><p>To pre-process your text simply means to bring your text into a form that is predictable  and analysable for your task. A task here is a combination of approach and domain. For example, extracting top keywords with TF-IDF (approach) from Tweets (domain) is an example of a Task.</p>

<pre><code>                  Task = approach + domain 



</code></pre>
<p>One task’s ideal pre-processing can become another task’s worst nightmare. So take note: text pre-processing is not directly transferable from task to task.</p>
<p>Let’s take a very simple example, let’s say you are trying to discover commonly used words in a news dataset. If your pre-processing step involves removing stopwords  because some other task used it, then you are probably going to miss out on some of the common words as you have ALREADY eliminated it. So really, it’s not a one-size-fits-all approach.</p>
<p>Here are some common pre-processing steps used in NLP software:</p>
<ul>
<li>Preliminaries:<br />
Sentence segmentation and word tokenization. </li>
</ul>
<ul>
<li>Frequent steps:<br />
Stop word removal, stemming and lemmatization, removing digits/punctuation, lowercasing, etc. </li>
</ul>
<ul>
<li>Advanced processing:<br />
POS tagging, parsing, coreference resolution, etc. </li>
</ul>
<h2 id="Preliminaries-:">Preliminaries :<a class="anchor-link" href="#Preliminaries-:"> </a></h2><p>While not all steps will be followed in all the NLP pipelines we encounter, the first two are more or less seen everywhere. Let’s take a look at what each of these steps mean.</p>
<p>The NLP can analysis the text by breaking it into sentence(sentence segmentation) and then further into words(words tokenization).</p>
<h3 id="SENTENCE-SEGMENTATION-:">SENTENCE SEGMENTATION :<a class="anchor-link" href="#SENTENCE-SEGMENTATION-:"> </a></h3><p>We may easily divide the text into sentence on the basis of the position of the full stop(.). But what happen if we have Dr.Joy or (….) in our text.</p>
<p>We have NLP libraries which help to overcome these issue. Like NLTK.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;It&#39;s fun to study NLP. Would recommend to all.&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sent_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
[&#34;It&#39;s fun to study NLP.&#34;, &#39;Would recommend to all.&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="WORD-TOKENIZATION-:">WORD TOKENIZATION :<a class="anchor-link" href="#WORD-TOKENIZATION-:"> </a></h3><p>To tokenize a sentence into words, we can start with a simple rule to split text into words based on the presence of punctuation marks. The NLTK library allows us to do that.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;It&#39;s fun to study NLP. Would recommend to all.&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;It&#39;, &#34;&#39;s&#34;, &#39;fun&#39;, &#39;to&#39;, &#39;study&#39;, &#39;NLP&#39;, &#39;.&#39;, &#39;Would&#39;, &#39;recommend&#39;, &#39;to&#39;, &#39;all&#39;, &#39;.&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Frequent-Steps-:">Frequent Steps :<a class="anchor-link" href="#Frequent-Steps-:"> </a></h2><p>Some frequent steps for pre-processing steps are:</p>
<ul>
<li>Lower casing </li>
<li>Removal of Punctuations </li>
<li>Removal of Stopwords </li>
<li>Removal of Frequent words </li>
<li>Stemming </li>
<li>Lemmatization </li>
<li>Removal of emojis </li>
<li>Removal of emoticons </li>
</ul>
<p>Well these steps are frequent but they may vary problem to problem. For eg let us consider we want to predict whether the given text belong to news, music or any other field. So for this problem we cannot remove the Frequent words like news article might contain the word news in it a lot, Hence to categorize the article we cannot just remove it.</p>
<p>As we encounter the word stemming and Lemmatization for the first time. Lets clear the meaning of these terms.</p>
<p>Stemming and Lemmatization helps us to achieve the root forms (sometimes called synonyms in search context) of inflected (derived) words.</p>
<h3 id="Stemming-:">Stemming :<a class="anchor-link" href="#Stemming-:"> </a></h3><p>Stemming is faster because it chops words without knowing the context of the words in given sentences.</p>
<ul>
<li>It is rule-based approach. </li>
<li>Accuracy is less. </li>
<li>When we convert any words into root-form then stemming mat create the non-existence meaning of a word. </li>
<li>Stemming is preferred when the meaning of the word is not important for analysis. EXAMPLE :- Spam Detection </li>
<li>For example : Studies =&gt; Studi </li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem.porter</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>
<span class="n">porter_stemmer</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>

<span class="n">word_data</span> <span class="o">=</span> <span class="s2">&quot;Da Vinci Code is such an amazing book to read.The book is full of suspense and Thriller. One of the best work of Dan Brown.&quot;</span>
<span class="c1"># First Word tokenization</span>
<span class="n">nltk_tokens</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">word_data</span><span class="p">)</span>
<span class="c1">#Next find the roots of the word</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">nltk_tokens</span><span class="p">:</span>
       <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Actual: </span><span class="si">%s</span><span class="s2">  Stem: </span><span class="si">%s</span><span class="s2">&quot;</span>  <span class="o">%</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">porter_stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">w</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Actual: Da  Stem: Da
Actual: Vinci  Stem: vinci
Actual: Code  Stem: code
Actual: is  Stem: is
Actual: such  Stem: such
Actual: an  Stem: an
Actual: amazing  Stem: amaz
Actual: book  Stem: book
Actual: to  Stem: to
Actual: read  Stem: read
Actual: .  Stem: .
Actual: .The  Stem: .the
Actual: book  Stem: book
Actual: si  Stem: si
Actual: full  Stem: full
Actual: of  Stem: of
Actual: suspense  Stem: suspens
Actual: and  Stem: and
Actual: Thriller  Stem: thriller
Actual: .  Stem: .
Actual: One  Stem: one
Actual: of  Stem: of
Actual: the  Stem: the
Actual: best  Stem: best
Actual: work  Stem: work
Actual: of  Stem: of
Actual: Dan  Stem: dan
Actual: Brown  Stem: brown
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Lemmatization-:">Lemmatization :<a class="anchor-link" href="#Lemmatization-:"> </a></h3><p>Lemmatization is slower as compared to stemming but it knows the context of the word before proceeding.</p>
<ul>
<li>It is a dictionary-based approach. </li>
<li>Accuracy is more as compared to Stemming. </li>
<li>Lemmatization always gives the dictionary meaning word while converting into root-form. </li>
<li>Lemmatization would be recommended when the meaning of the word is important for analysis.Example: Question Answer 
*For Example: “Studies” =&gt; “Study” </li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>
<span class="n">wordnet_lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>

<span class="n">word_data</span> <span class="o">=</span> <span class="s2">&quot;Da Vinci Code is such an amazing book to read.The book is full of suspense and Thriller. One of the best work of Dan Brown.&quot;</span>
<span class="n">nltk_tokens</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">word_data</span><span class="p">)</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">nltk_tokens</span><span class="p">:</span>
       <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Actual: </span><span class="si">%s</span><span class="s2">  Lemma: </span><span class="si">%s</span><span class="s2">&quot;</span>  <span class="o">%</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">wordnet_lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">w</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Actual: Da  Lemma: Da
Actual: Vinci  Lemma: Vinci
Actual: Code  Lemma: Code
Actual: is  Lemma: is
Actual: such  Lemma: such
Actual: an  Lemma: an
Actual: amazing  Lemma: amazing
Actual: book  Lemma: book
Actual: to  Lemma: to
Actual: read.The  Lemma: read.The
Actual: book  Lemma: book
Actual: is  Lemma: is
Actual: full  Lemma: full
Actual: of  Lemma: of
Actual: suspense  Lemma: suspense
Actual: and  Lemma: and
Actual: Thriller  Lemma: Thriller
Actual: .  Lemma: .
Actual: One  Lemma: One
Actual: of  Lemma: of
Actual: the  Lemma: the
Actual: best  Lemma: best
Actual: work  Lemma: work
Actual: of  Lemma: of
Actual: Dan  Lemma: Dan
Actual: Brown  Lemma: Brown
Actual: .  Lemma: .
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Some other pre-processing steps that are not that common are:</p>
<h3 id="Text-Normalization:">Text Normalization:<a class="anchor-link" href="#Text-Normalization:"> </a></h3><p>Text normalization is the process of transforming a text into a canonical (standard) form. For example, the word “gooood” and “gud” can be transformed to “good”, its canonical form. Another example is mapping of near identical words such as “stopwords”, “stop-words” and “stop words” to just “stopwords”.</p>
<h3 id="Language-Detection:">Language Detection:<a class="anchor-link" href="#Language-Detection:"> </a></h3><p>Well what happen if our text is in other language apart from English. Our whole pipeline will accept an English  text. So for that we need to detect the language before creating the pipeline.</p>
<h2 id="Advance-Processing">Advance Processing<a class="anchor-link" href="#Advance-Processing"> </a></h2><h3 id="POS-Tagging-:">POS Tagging :<a class="anchor-link" href="#POS-Tagging-:"> </a></h3><p>Imagine we’re asked to develop a system to identify person and organization names in our company’s collection of one million documents. The common pre-processing steps we discussed earlier may not be relevant in this context. Identifying names requires us to be able to do POS tagging, as identifying proper nouns can be useful in identifying person and organization names. Pre-trained and readily usable POS taggers are implemented in NLP libraries such as NLTK, spaCy and Parsey McParseface Tagger.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="s2">&quot;The quick brown fox jumps over a lazy dog&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Part of speech&quot;</span><span class="p">,</span><span class="n">nltk</span><span class="o">.</span><span class="n">pos_tag</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Part of speech [(&#39;The&#39;, &#39;DT&#39;), (&#39;quick&#39;, &#39;JJ&#39;), (&#39;brown&#39;, &#39;NN&#39;), (&#39;fox&#39;, &#39;NN&#39;), (&#39;jumps&#39;, &#39;VBZ&#39;), (&#39;over&#39;, &#39;IN&#39;), (&#39;a&#39;, &#39;DT&#39;), (&#39;lazy&#39;, &#39;JJ&#39;), (&#39;dog&#39;, &#39;NN&#39;)]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Parse-Tree:">Parse Tree:<a class="anchor-link" href="#Parse-Tree:"> </a></h3><p>Now if we have to find the relationship between person and organization then  we need to anayisis the senentce in depth and for that  parse tree play a major role. Parse tree is a tree representation of different syntactic categories of a sentence. It helps us to understand the syntactical structure of a sentence.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">pos_tag</span><span class="p">,</span> <span class="n">word_tokenize</span><span class="p">,</span> <span class="n">RegexpParser</span> 
   
<span class="c1"># Example text </span>
<span class="n">sample_text</span> <span class="o">=</span> <span class="s2">&quot;The quick brown fox jumps over the lazy dog&quot;</span>
   
<span class="c1"># Find all parts of speech in above sentence </span>
<span class="n">tagged</span> <span class="o">=</span> <span class="n">pos_tag</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sample_text</span><span class="p">))</span> 
   
<span class="c1">#Extract all parts of speech from any text </span>
<span class="n">chunker</span> <span class="o">=</span> <span class="n">RegexpParser</span><span class="p">(</span><span class="s2">&quot;&quot;&quot; </span>
<span class="s2">                       NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;}    #To extract Noun Phrases </span>
<span class="s2">                       P: {&lt;IN&gt;}               #To extract Prepositions </span>
<span class="s2">                       V: {&lt;V.*&gt;}              #To extract Verbs </span>
<span class="s2">                       PP: {&lt;P&gt; &lt;NP&gt;}          #To extract Prepostional Phrases </span>
<span class="s2">                       VP: {&lt;V&gt; &lt;NP|PP&gt;*}      #To extarct Verb Phrases </span>
<span class="s2">                       &quot;&quot;&quot;</span><span class="p">)</span> 
  
<span class="c1"># Print all parts of speech in above sentence </span>
<span class="n">output</span> <span class="o">=</span> <span class="n">chunker</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">tagged</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After Extracting</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> 
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>After Extracting
 (S
  (NP The/DT quick/JJ brown/NN)
  (NP fox/NN)
  (VP (V jumps/VBZ) (PP (P over/IN) (NP the/DT lazy/JJ dog/NN))))
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Conference-Resolution-:">Conference Resolution :<a class="anchor-link" href="#Conference-Resolution-:"> </a></h3><p>Coreference resolution is the task of finding all expressions that refer to the same entity in a text.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="FEATURE-ENGINEERING:">FEATURE ENGINEERING:<a class="anchor-link" href="#FEATURE-ENGINEERING:"> </a></h1><p>When we use ML methods to perform our modeling step later, we’ll still need a way to feed this pre-processed text into an ML algorithm. Feature engineering refers to the set of methods that will accomplish this task. It’s also referred to as feature extraction. The goal of feature engineering is to capture the characteristics of the text into a numeric vector that can be understood by the ML algorithms.</p>
<p>two different approaches taken in practice for feature engineering in</p>
<h2 id="classical-NLP-and-traditional-ML-pipeline">classical NLP and traditional ML pipeline<a class="anchor-link" href="#classical-NLP-and-traditional-ML-pipeline"> </a></h2><p>Feature engineering is an integral step in any ML pipeline. Feature engineering steps convert the raw data into a format that can be consumed by a machine. These transformation functions are usually handcrafted in the classical ML pipeline, aligning to the task at hand. For example, imagine a task of sentiment classification on product reviews in e-commerce. One way to convert the reviews into meaningful “numbers” that helps predict the reviews’ sentiments (positive or negative) would be to count the number of positive and negative words in each review. There are statistical measures for understanding if a feature is useful for a task or not.</p>
<p>One of the advantages of handcrafted features is that the model remains interpretable—it’s possible to quantify exactly how much each feature is influencing the model prediction.</p>
<h2 id="DL-pipeline">DL pipeline<a class="anchor-link" href="#DL-pipeline"> </a></h2><p>In the DL pipeline, the raw data (after pre-processing) is directly fed to a model.The model is capable of “learning” features from the data. Hence, these features are more in line with the task at hand, so they generally give improved performance. But, since all these features are learned via model parameters, the model loses interpretability.</p>
<h1 id="RECAP:">RECAP:<a class="anchor-link" href="#RECAP:"> </a></h1><p>The first step in the process of developing any NLP system is to collect data relevant to the given task. Even if we’re building a rule-based system, we still need some data to design and test our rules. The data we get is seldom(rarely) clean, and this is where text cleaning comes into play. After cleaning, text data often has a lot of variations and needs to be converted into a canonical (principle or a pre-defined way) form. This is done in the pre-processing step. This is followed by feature engineering, where we carve out indicators that are most suitable for the task at hand.</p>
<p>These indicators/features are converted into a format that is understandable by modeling algorithms.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><div class="footnotes"><p id="fn-2">2. Notes are compiled from <a href="https://www.oreilly.com/library/view/practical-natural-language/9781492054047/"> Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems</a>, <a href="https://www.geeksforgeeks.org/syntax-tree-natural-language-processing/">GeeksforGeeks</a>, <a href="https://www.tutorialspoint.com/python_data_science/python_stemming_and_lemmatization.htm">tutorialspoint-Stemming and Lemmatization</a>, <a href="https://www.geeksforgeeks.org/syntax-tree-natural-language-processing/">gfg-parse tree</a>, <a href="https://morioh.com/p/a7b8982e5a5a">morioh</a> and <a href="https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b">Medium-Tokenization and Parts of Speech(POS) Tagging in Python’s NLTK library</a><a href="#fnref-2" class="footnote footnotes">↩</a></p></div>
<div class="footnotes"><p id="fn-3">3. If you face any problem or have any feedback/suggestions feel free to comment.<a href="#fnref-3" class="footnote footnotes">↩</a></p></div></p>

</div>
</div>
</div>
&lt;/div&gt;
 

</INSERT></div></div></div></div>


  </div><a class="u-url" href="/Bits-and-Bytes-of-NLP/2020/12/23/_12_24_Building_An_NLP_Pipeline_(PART_1).html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/Bits-and-Bytes-of-NLP/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/Bits-and-Bytes-of-NLP/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/Bits-and-Bytes-of-NLP/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A technical blog which cover all Bits and Bytes of NLP.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/ShubhangiDabral13" title="ShubhangiDabral13"><svg class="svg-icon grey"><use xlink:href="/Bits-and-Bytes-of-NLP/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/Shubhi_Dabral" title="Shubhi_Dabral"><svg class="svg-icon grey"><use xlink:href="/Bits-and-Bytes-of-NLP/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
